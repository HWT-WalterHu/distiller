/home/hwt/anaconda2/envs/distiller/bin/python /home/hwt/pyprojects/distiller/examples/classifier_compression/compress_classifier.py -h
usage: compress_classifier.py [-h] [--arch ARCH] [-j N] [--epochs N] [-b N]
                              [--lr LR] [--momentum M] [--weight-decay W]
                              [--print-freq N] [--resume PATH] [-e]
                              [--pretrained]
                              [--activation-stats PHASE [PHASE ...]]
                              [--masks-sparsity] [--param-hist]
                              [--summary {sparsity,compute,model,modules,png,png_w_params,onnx}]
                              [--compress [COMPRESS]]
                              [--sense {element,filter,channel}]
                              [--sense-range SENSITIVITY_RANGE SENSITIVITY_RANGE SENSITIVITY_RANGE]
                              [--extras EXTRAS] [--deterministic]
                              [--gpus DEV_ID] [--cpu] [--name NAME]
                              [--out-dir OUTPUT_DIR]
                              [--validation-split VALIDATION_SPLIT]
                              [--effective-train-size EFFECTIVE_TRAIN_SIZE]
                              [--effective-valid-size EFFECTIVE_VALID_SIZE]
                              [--effective-test-size EFFECTIVE_TEST_SIZE]
                              [--confusion]
                              [--earlyexit_lossweights [EARLYEXIT_LOSSWEIGHTS [EARLYEXIT_LOSSWEIGHTS ...]]]
                              [--earlyexit_thresholds [EARLYEXIT_THRESHOLDS [EARLYEXIT_THRESHOLDS ...]]]
                              [--num-best-scores NUM_BEST_SCORES]
                              [--load-serialized] [--thinnify]
                              [--kd-teacher ARCH] [--kd-pretrained]
                              [--kd-resume PATH] [--kd-temperature TEMP]
                              [--kd-distill-wt WEIGHT]
                              [--kd-student-wt WEIGHT]
                              [--kd-teacher-wt WEIGHT]
                              [--kd-start-epoch EPOCH_NUM]
                              [--quantize-eval | --qe-calibration PORTION_OF_TEST_SET]
                              [--qe-mode QE_MODE] [--qe-bits-acts NUM_BITS]
                              [--qe-bits-wts NUM_BITS]
                              [--qe-bits-accum NUM_BITS] [--qe-clip-acts]
                              [--qe-no-clip-layers LAYER_NAME [LAYER_NAME ...]]
                              [--qe-per-channel] [--qe-stats-file PATH]
                              [--qe-config-file PATH] [--greedy]
                              [--greedy-ft-epochs GREEDY_FT_EPOCHS]
                              [--greedy-target-density GREEDY_TARGET_DENSITY]
                              [--greedy-pruning-step GREEDY_PRUNING_STEP]
                              [--greedy-finetuning-policy {constant,linear-grow}]
                              [--amc]
                              [--amc-protocol {mac-constrained,param-constrained,accuracy-guaranteed,mac-constrained-experimental}]
                              [--amc-ft-epochs AMC_FT_EPOCHS]
                              [--amc-save-chkpts]
                              [--amc-action-range AMC_ACTION_RANGE AMC_ACTION_RANGE]
                              [--amc-heatup-epochs AMC_HEATUP_EPOCHS]
                              [--amc-training-epochs AMC_TRAINING_EPOCHS]
                              [--amc-reward-frequency AMC_REWARD_FREQUENCY]
                              [--amc-target-density AMC_TARGET_DENSITY]
                              [--amc-agent-algo {ClippedPPO-continuous,ClippedPPO-discrete,DDPG,Random-policy}]
                              [--amc-ft-frequency AMC_FT_FREQUENCY]
                              DIR

Distiller image classification model compression

positional arguments:
  DIR                   path to dataset

optional arguments:
  -h, --help            show this help message and exit
  --arch ARCH, -a ARCH  model architecture: alexnet | alexnet_bn | densenet121
                        | densenet161 | densenet169 | densenet201 |
                        inception_v3 | mobilenet | mobilenet_025 |
                        mobilenet_050 | mobilenet_075 | plain20_cifar |
                        preact_resnet101 | preact_resnet110_cifar |
                        preact_resnet110_cifar_conv_ds | preact_resnet152 |
                        preact_resnet18 | preact_resnet20_cifar |
                        preact_resnet20_cifar_conv_ds | preact_resnet32_cifar
                        | preact_resnet32_cifar_conv_ds | preact_resnet34 |
                        preact_resnet44_cifar | preact_resnet44_cifar_conv_ds
                        | preact_resnet50 | preact_resnet56_cifar |
                        preact_resnet56_cifar_conv_ds | resnet101 |
                        resnet101_earlyexit | resnet110_cifar_earlyexit |
                        resnet1202_cifar_earlyexit | resnet152 |
                        resnet152_earlyexit | resnet18 | resnet18_earlyexit |
                        resnet20_cifar | resnet20_cifar_earlyexit |
                        resnet32_cifar | resnet32_cifar_earlyexit | resnet34 |
                        resnet34_earlyexit | resnet44_cifar |
                        resnet44_cifar_earlyexit | resnet50 |
                        resnet50_earlyexit | resnet56_cifar |
                        resnet56_cifar_earlyexit | simplenet_cifar |
                        squeezenet1_0 | squeezenet1_1 | vgg11 | vgg11_bn |
                        vgg11_bn_cifar | vgg11_cifar | vgg13 | vgg13_bn |
                        vgg13_bn_cifar | vgg13_cifar | vgg16 | vgg16_bn |
                        vgg16_bn_cifar | vgg16_cifar | vgg19 | vgg19_bn |
                        vgg19_bn_cifar | vgg19_cifar (default: resnet18)
  -j N, --workers N     number of data loading workers (default: 4)
  --epochs N            number of total epochs to run
  -b N, --batch-size N  mini-batch size (default: 256)
  --lr LR, --learning-rate LR
                        initial learning rate
  --momentum M          momentum
  --weight-decay W, --wd W
                        weight decay (default: 1e-4)
  --print-freq N, -p N  print frequency (default: 10)
  --resume PATH         path to latest checkpoint (default: none)
  -e, --evaluate        evaluate model on validation set
  --pretrained          use pre-trained model
  --activation-stats PHASE [PHASE ...], --act-stats PHASE [PHASE ...]
                        collect activation statistics on phases: train, valid,
                        and/or test (WARNING: this slows down training)
  --masks-sparsity      print masks sparsity table at end of each epoch
  --param-hist          log the parameter tensors histograms to file (WARNING:
                        this can use significant disk space)
  --summary {sparsity,compute,model,modules,png,png_w_params,onnx}
                        print a summary of the model, and exit - options:
                        sparsity | compute | model | modules | png |
                        png_w_params | onnx
  --compress [COMPRESS]
                        configuration file for pruning the model (default is
                        to use hard-coded schedule)
  --sense {element,filter,channel}
                        test the sensitivity of layers to pruning
  --sense-range SENSITIVITY_RANGE SENSITIVITY_RANGE SENSITIVITY_RANGE
                        an optional parameter for sensitivity testing
                        providing the range of sparsities to test. This is
                        equivalent to creating sensitivities =
                        np.arange(start, stop, step)
  --extras EXTRAS       file with extra configuration information
  --deterministic, --det
                        Ensure deterministic execution for re-producible
                        results.
  --gpus DEV_ID         Comma-separated list of GPU device IDs to be used
                        (default is to use all available devices)
  --cpu                 Use CPU only. Flag not set => uses GPUs according to
                        the --gpus flag value.Flag set => overrides the --gpus
                        flag
  --name NAME, -n NAME  Experiment name
  --out-dir OUTPUT_DIR, -o OUTPUT_DIR
                        Path to dump logs and checkpoints
  --validation-split VALIDATION_SPLIT, --valid-size VALIDATION_SPLIT, --vs VALIDATION_SPLIT
                        Portion of training dataset to set aside for
                        validation
  --effective-train-size EFFECTIVE_TRAIN_SIZE, --etrs EFFECTIVE_TRAIN_SIZE
                        Portion of training dataset to be used in each epoch.
                        NOTE: If --validation-split is set, then the value of
                        this argument is applied AFTER the train-validation
                        split according to that argument
  --effective-valid-size EFFECTIVE_VALID_SIZE, --evs EFFECTIVE_VALID_SIZE
                        Portion of validation dataset to be used in each
                        epoch. NOTE: If --validation-split is set, then the
                        value of this argument is applied AFTER the train-
                        validation split according to that argument
  --effective-test-size EFFECTIVE_TEST_SIZE, --etes EFFECTIVE_TEST_SIZE
                        Portion of test dataset to be used in each epoch
  --confusion           Display the confusion matrix
  --earlyexit_lossweights [EARLYEXIT_LOSSWEIGHTS [EARLYEXIT_LOSSWEIGHTS ...]]
                        List of loss weights for early exits (e.g.
                        --earlyexit_lossweights 0.1 0.3)
  --earlyexit_thresholds [EARLYEXIT_THRESHOLDS [EARLYEXIT_THRESHOLDS ...]]
                        List of EarlyExit thresholds (e.g.
                        --earlyexit_thresholds 1.2 0.9)
  --num-best-scores NUM_BEST_SCORES
                        number of best scores to track and report (default: 1)
  --load-serialized     Load a model without DataParallel wrapping it
  --thinnify            physically remove zero-filters and create a smaller
                        model
  --greedy              greedy filter pruning
  --amc                 AutoML Compression

Knowledge Distillation Training Arguments:
  --kd-teacher ARCH     Model architecture for teacher model
  --kd-pretrained       Use pre-trained model for teacher
  --kd-resume PATH      Path to checkpoint from which to load teacher weights
  --kd-temperature TEMP, --kd-temp TEMP
                        Knowledge distillation softmax temperature
  --kd-distill-wt WEIGHT, --kd-dw WEIGHT
                        Weight for distillation loss (student vs. teacher soft
                        targets)
  --kd-student-wt WEIGHT, --kd-sw WEIGHT
                        Weight for student vs. labels loss
  --kd-teacher-wt WEIGHT, --kd-tw WEIGHT
                        Weight for teacher vs. labels loss
  --kd-start-epoch EPOCH_NUM
                        Epoch from which to enable distillation

Arguments controlling quantization at evaluation time ("post-training quantization"):
  --quantize-eval, --qe
                        Apply linear quantization to model before evaluation.
                        Applicable only if --evaluate is also set
  --qe-calibration PORTION_OF_TEST_SET
                        Run the model in evaluation mode on the specified
                        portion of the test dataset and collect statistics.
                        Ignores all other 'qe--*' arguments
  --qe-mode QE_MODE, --qem QE_MODE
                        Linear quantization mode. Choices: sym | asym_s |
                        asym_u
  --qe-bits-acts NUM_BITS, --qeba NUM_BITS
                        Number of bits for quantization of activations
  --qe-bits-wts NUM_BITS, --qebw NUM_BITS
                        Number of bits for quantization of weights
  --qe-bits-accum NUM_BITS
                        Number of bits for quantization of the accumulator
  --qe-clip-acts, --qeca
                        Enable clipping of activations using min/max values
                        averaging over batch
  --qe-no-clip-layers LAYER_NAME [LAYER_NAME ...], --qencl LAYER_NAME [LAYER_NAME ...]
                        List of layer names for which not to clip activations.
                        Applicable only if --qe-clip-acts is also set
  --qe-per-channel, --qepc
                        Enable per-channel quantization of weights (per output
                        channel)
  --qe-stats-file PATH  Path to YAML file with calibration stats. If not
                        given, dynamic quantization will be run (Note that not
                        all layer types are supported for dynamic
                        quantization)
  --qe-config-file PATH
                        Path to YAML file containing configuration for
                        PostTrainLinearQuantizer (if present, all other --qe*
                        arguments are ignored)

Greedy Pruning:
  --greedy-ft-epochs GREEDY_FT_EPOCHS
                        number of epochs to fine-tune each discovered network
  --greedy-target-density GREEDY_TARGET_DENSITY
                        target density of the network we are seeking
  --greedy-pruning-step GREEDY_PRUNING_STEP
                        size of each pruning step (as a fraction in [0..1])
  --greedy-finetuning-policy {constant,linear-grow}
                        policy used for determining how long to fine-tune

AutoML Compression Arguments:
  --amc-protocol {mac-constrained,param-constrained,accuracy-guaranteed,mac-constrained-experimental}
                        Compression-policy search protocol
  --amc-ft-epochs AMC_FT_EPOCHS
                        The number of epochs to fine-tune each discovered
                        network
  --amc-save-chkpts     Save checkpoints of all discovered networks
  --amc-action-range AMC_ACTION_RANGE AMC_ACTION_RANGE
                        Density action range (a_min, a_max)
  --amc-heatup-epochs AMC_HEATUP_EPOCHS
                        The number of epochs for heatup/exploration
  --amc-training-epochs AMC_TRAINING_EPOCHS
                        The number of epochs for training/exploitation
  --amc-reward-frequency AMC_REWARD_FREQUENCY
                        Reward computation frequency (measured in agent steps)
  --amc-target-density AMC_TARGET_DENSITY
                        Target density of the network we are seeking
  --amc-agent-algo {ClippedPPO-continuous,ClippedPPO-discrete,DDPG,Random-policy}
                        The agent algorithm to use
  --amc-ft-frequency AMC_FT_FREQUENCY
                        How many action-steps between fine-tuning. By default
                        there is no fine-tuning between steps.

-a simplenet_cifar --epochs 80 -n KDR20SimpleNet_slr --gpus 7 ../../../data.cifar10 --compress traincifar.yaml --kd-teacher resnet20_cifar
-a simplenet_cifar --epochs 50 -n R20simpletnet_slr --gpus 7 ../../../data.cifar10 --compress traincifar.yaml --kd-teacher resnet20_cifar --kd-pretrained --kd-distill-wt 0.1 --kd-student-wt 0.9 --kd-temp 10.0